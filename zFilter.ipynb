{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a19c262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.program import LLMTextCompletionProgram\n",
    "from llama_index.core.output_parsers import PydanticOutputParser\n",
    "from llama_index.core.vector_stores import ExactMatchFilter, MetadataFilters\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "import logging\n",
    "from llama_index.core.node_parser import (\n",
    "    SemanticSplitterNodeParser,\n",
    "    HierarchicalNodeParser\n",
    ")\n",
    "from llama_index.core import (\n",
    "    Settings,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    SimpleDirectoryReader, load_index_from_storage,\n",
    ")\n",
    "from llama_index.core.node_parser import get_leaf_nodes, get_root_nodes\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "from llama_index.embeddings.bedrock import BedrockEmbedding\n",
    "from llama_index.llms.bedrock import Bedrock\n",
    "from llama_index.core import Settings\n",
    "\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.extractors import (\n",
    "    SummaryExtractor,\n",
    "    QuestionsAnsweredExtractor,\n",
    "    TitleExtractor,\n",
    "    KeywordExtractor,\n",
    "    BaseExtractor,\n",
    ")\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core.question_gen import LLMQuestionGenerator\n",
    "from llama_index.core.question_gen.prompts import (\n",
    "    DEFAULT_SUB_QUESTION_PROMPT_TMPL,\n",
    ")\n",
    "from llama_index.core.retrievers import AutoMergingRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from qdrant_client import QdrantClient\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import json\n",
    "import os\n",
    "import boto3\n",
    "import docx, jaconv, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e53beb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash \"HTTP/1.1 200 OK\"\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "llm = GoogleGenAI(\n",
    "    model_name=\"gemini-2.0-flash\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "embed_model = BedrockEmbedding(\n",
    "    model_name=\"amazon.titan-embed-text-v2:0\",\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    ")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff453eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Assume you have a function to get valid filters from your DB\n",
    "def get_valid_filters_from_db():\n",
    "    # In a real application, this would query your PostgreSQL database\n",
    "    # SELECT DISTINCT \"Category\" FROM ...; SELECT DISTINCT \"Keywords\" FROM ...;\n",
    "    return {\n",
    "        \"categories\": [\"Customer Value Analysis\", \"Loyalty & Retention\", \"Case Study\", \"Branding & Positioning\"],\n",
    "        \"keywords\": [\"Customer Lifetime Value (CLV)\", \"Churn Rate Analysis\", \"Marketing ROI\", \"Decile Analysis\"]\n",
    "    }\n",
    "\n",
    "# Define the Pydantic model for the structured output you want from the LLM\n",
    "class QueryFilters(BaseModel):\n",
    "    \"\"\"Data model for query filters.\"\"\"\n",
    "    categories: List[str] = Field(description=\"List of categories to filter on.\")\n",
    "    keywords: List[str] = Field(description=\"List of keywords to filter on.\")\n",
    "\n",
    "# --- This function replaces the need for the complex prompt in _create_custom_auto_retriever ---\n",
    "def generate_filters_from_query(query_str: str, llm):\n",
    "    \"\"\"\n",
    "    Uses an LLM to extract relevant categories and keywords from a user query.\n",
    "    \"\"\"\n",
    "    logger.debug(f\"Generating filters for user query: {query_str}\")\n",
    "    \n",
    "    # Get the latest valid filters from the database\n",
    "    valid_filters = get_valid_filters_from_db()\n",
    "    \n",
    "    # Create a new, simpler prompt focused only on extracting filters\n",
    "    prompt_template_str = f\"\"\"\n",
    "    Based on the user's query, identify the most relevant categories and keywords to use for filtering documents.\n",
    "    \n",
    "    Respond ONLY with a JSON object.\n",
    "    \n",
    "    Available Categories: {valid_filters['categories']}\n",
    "    Available Keywords: {valid_filters['keywords']}\n",
    "    \n",
    "    User Query: \"{query_str}\"\n",
    "    \n",
    "    JSON Output:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define a program to get structured output from the LLM\n",
    "    program = LLMTextCompletionProgram.from_defaults(\n",
    "        output_parser=PydanticOutputParser(output_cls=QueryFilters),\n",
    "        prompt_template_str=prompt_template_str,\n",
    "        llm=llm,\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    # Run the program to get the structured filter object\n",
    "    filter_object = program()\n",
    "    \n",
    "    logger.info(f\"LLM generated filters: Categories={filter_object.categories}, Keywords={filter_object.keywords}\")\n",
    "    return filter_object\n",
    "\n",
    "def create_query_engine(self, persist_dir: str = \"./storage_testing\", similarity_top_k: int = 30):\n",
    "    \"\"\"\n",
    "    Creates a Query Engine that first generates filters and then retrieves data.\n",
    "    \"\"\"\n",
    "    logger.debug(\"Creating dynamic query engine...\")\n",
    "    \n",
    "    if not self.index:\n",
    "        self.build_automerging_index(persist_dir)\n",
    "\n",
    "    # Let's imagine a user asks a question\n",
    "    user_query = \"How can I analyze Marketing ROI for customer retention campaigns?\"\n",
    "\n",
    "    # 1. Generate filters using the LLM\n",
    "    # You would need to pass your LLM instance here\n",
    "    generated_filters = generate_filters_from_query(user_query, self.llm) \n",
    "\n",
    "    # 2. Construct the metadata filters dynamically\n",
    "    dynamic_filters = []\n",
    "    if generated_filters.categories:\n",
    "        cat_filters = [ExactMatchFilter(key=\"Category\", value=c) for c in generated_filters.categories]\n",
    "        dynamic_filters.extend(cat_filters)\n",
    "    \n",
    "    if generated_filters.keywords:\n",
    "        kw_filters = [ExactMatchFilter(key=\"Keywords\", value=k) for k in generated_filters.keywords]\n",
    "        dynamic_filters.extend(kw_filters)\n",
    "\n",
    "    # 3. Create the base retriever with the DYNAMIC filters\n",
    "    base_retriever = self.index.as_retriever(\n",
    "        similarity_top_k=similarity_top_k,\n",
    "        filters=MetadataFilters(filters=dynamic_filters)\n",
    "    )\n",
    "\n",
    "    # 4. Create the AutoMerging Retriever as before\n",
    "    retriever = AutoMergingRetriever(base_retriever, \n",
    "                                     self.storage_context,\n",
    "                                     verbose=True)\n",
    "    \n",
    "    # 5. Create the final Query Engine\n",
    "    query_engine = RetrieverQueryEngine.from_args(\n",
    "        retriever=retriever,\n",
    "        response_mode=ResponseMode.TREE_SUMMARIZE,\n",
    "        callback_manager=CallbackManager([LlamaDebugHandler(print_trace_on_end=True)])\n",
    "    )\n",
    "    \n",
    "    logger.debug(\"Dynamic query engine is ready!\")\n",
    "\n",
    "\n",
    "    return query_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e299d5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:__main__:LLM generated filters: Categories=['Case Study'], Keywords=['Marketing ROI', 'Decile Analysis']\n"
     ]
    }
   ],
   "source": [
    "a = generate_filters_from_query(query_str=\"Give me some case study on marketing ROI and decile analysis\", llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b429cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== KHỞI TẠO NODE STORAGE HANDLER ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash \"HTTP/1.1 200 OK\"\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:__main__:Thiết lập Qdrant client...\n",
      "INFO:httpx:HTTP Request: GET https://76cc3790-f85c-4c86-9b1e-6e9864e43843.us-east4-0.gcp.cloud.qdrant.io:6333 \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Đã kết nối với Qdrant Cloud\n",
      "INFO:__main__:Kiểm tra kết nối...\n",
      "INFO:httpx:HTTP Request: GET https://76cc3790-f85c-4c86-9b1e-6e9864e43843.us-east4-0.gcp.cloud.qdrant.io:6333/collections \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:✅ Qdrant connection OK - 3 collections\n",
      "INFO:__main__:Thiết lập complete pipeline...\n",
      "INFO:__main__:Xây dựng AutoMerging Index...\n",
      "INFO:__main__:Đang thử tải index hiện có...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== THIẾT LẬP COMPLETE PIPELINE ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://76cc3790-f85c-4c86-9b1e-6e9864e43843.us-east4-0.gcp.cloud.qdrant.io:6333/collections/sailing/exists \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://76cc3790-f85c-4c86-9b1e-6e9864e43843.us-east4-0.gcp.cloud.qdrant.io:6333/collections/sailing \"HTTP/1.1 200 OK\"\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "INFO:__main__:Đã tải index thành công.\n",
      "INFO:__main__:Tạo query engine...\n",
      "INFO:__main__:Tạo query engine...\n",
      "INFO:__main__:DynamicQueryWrapper initialized.\n",
      "INFO:__main__:Complete pipeline đã sẵn sàng!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./storage\\docstore.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./storage\\index_store.json.\n",
      "\n",
      "==================================================\n",
      "=== CHẾ ĐỘ INTERACTIVE QUERY ===\n",
      "Nhập câu hỏi của bạn (gõ 'quit' để thoát):\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Executing dynamic query: 'Tell me something about decile analysis'\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Đang tìm kiếm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_6632\\3121455676.py:78: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  logger.info(f\"LLM generated filters: {filter_object.dict()}\")\n",
      "INFO:__main__:LLM generated filters: {'categories': [], 'keywords': ['Decile Analysis']}\n",
      "INFO:httpx:HTTP Request: POST https://76cc3790-f85c-4c86-9b1e-6e9864e43843.us-east4-0.gcp.cloud.qdrant.io:6333/collections/sailing/points/search \"HTTP/1.1 400 Bad Request\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: query\n",
      "    |_CBEventType.QUERY -> 2.744161 seconds\n",
      "**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Lỗi khi xử lý câu hỏi: Unexpected Response: 400 (Bad Request)\n",
      "Raw response content:\n",
      "b'{\"status\":{\"error\":\"Bad request: Index required but not found for \\\\\"Keywords\\\\\" of one of the following types: [keyword]. Help: Create an index for this key or use a different filter.\"},\"time\":3.639 ...'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_6632\\3121455676.py\", line 315, in main\n",
      "    response = query_engine.query(user_query)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_6632\\3121455676.py\", line 130, in query\n",
      "    return query_engine_instance.query(query_str)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\Desktop\\Advanced-RAG\\env\\Lib\\site-packages\\llama_index_instrumentation\\dispatcher.py\", line 317, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\Desktop\\Advanced-RAG\\env\\Lib\\site-packages\\llama_index\\core\\base\\base_query_engine.py\", line 52, in query\n",
      "    dispatcher.event(QueryStartEvent(query=str_or_query_bundle))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\Desktop\\Advanced-RAG\\env\\Lib\\site-packages\\llama_index_instrumentation\\dispatcher.py\", line 317, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\Desktop\\Advanced-RAG\\env\\Lib\\site-packages\\llama_index\\core\\query_engine\\retriever_query_engine.py\", line 193, in _query\n",
      "    nodes = self.retrieve(query_bundle)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\Desktop\\Advanced-RAG\\env\\Lib\\site-packages\\llama_index\\core\\query_engine\\retriever_query_engine.py\", line 146, in retrieve\n",
      "    nodes = self._retriever.retrieve(query_bundle)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\Desktop\\Advanced-RAG\\env\\Lib\\site-packages\\llama_index_instrumentation\\dispatcher.py\", line 317, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\Desktop\\Advanced-RAG\\env\\Lib\\site-packages\\llama_index\\core\\base\\base_retriever.py\", line 246, in retrieve\n",
      "    payload={EventPayload.NODES: nodes},\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\Desktop\\Advanced-RAG\\env\\Lib\\site-packages\\llama_index_instrumentation\\dispatcher.py\", line 317, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\Desktop\\Advanced-RAG\\env\\Lib\\site-packages\\llama_index\\core\\retrievers\\auto_merging_retriever.py\", line 183, in _retrieve\n",
      "    initial_nodes = self._vector_retriever.retrieve(query_bundle)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\Desktop\\Advanced-RAG\\env\\Lib\\site-packages\\llama_index_instrumentation\\dispatcher.py\", line 317, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\Desktop\\Advanced-RAG\\env\\Lib\\site-packages\\llama_index\\core\\base\\base_retriever.py\", line 246, in retrieve\n",
      "    payload={EventPayload.NODES: nodes},\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\Desktop\\Advanced-RAG\\env\\Lib\\site-packages\\llama_index_instrumentation\\dispatcher.py\", line 317, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\Desktop\\Advanced-RAG\\env\\Lib\\site-packages\\llama_index\\core\\indices\\vector_store\\retrievers\\retriever.py\", line 104, in _retrieve\n",
      "    return self._get_nodes_with_embeddings(query_bundle)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\Desktop\\Advanced-RAG\\env\\Lib\\site-packages\\llama_index\\core\\indices\\vector_store\\retrievers\\retriever.py\", line 220, in _get_nodes_with_embeddings\n",
      "    query_result = self._vector_store.query(query, **self._kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\Desktop\\Advanced-RAG\\env\\Lib\\site-packages\\llama_index\\vector_stores\\qdrant\\base.py\", line 908, in query\n",
      "    response = self._client.search(\n",
      "               ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\Desktop\\Advanced-RAG\\env\\Lib\\site-packages\\qdrant_client\\qdrant_client.py\", line 376, in search\n",
      "    return self._client.search(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\Desktop\\Advanced-RAG\\env\\Lib\\site-packages\\qdrant_client\\qdrant_remote.py\", line 523, in search\n",
      "    search_result = self.http.search_api.search_points(\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\Desktop\\Advanced-RAG\\env\\Lib\\site-packages\\qdrant_client\\http\\api\\search_api.py\", line 936, in search_points\n",
      "    return self._build_for_search_points(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\Desktop\\Advanced-RAG\\env\\Lib\\site-packages\\qdrant_client\\http\\api\\search_api.py\", line 487, in _build_for_search_points\n",
      "    return self.api_client.request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\Desktop\\Advanced-RAG\\env\\Lib\\site-packages\\qdrant_client\\http\\api_client.py\", line 95, in request\n",
      "    return self.send(request, type_)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\Desktop\\Advanced-RAG\\env\\Lib\\site-packages\\qdrant_client\\http\\api_client.py\", line 130, in send\n",
      "    raise UnexpectedResponse.for_response(response)\n",
      "qdrant_client.http.exceptions.UnexpectedResponse: Unexpected Response: 400 (Bad Request)\n",
      "Raw response content:\n",
      "b'{\"status\":{\"error\":\"Bad request: Index required but not found for \\\\\"Keywords\\\\\" of one of the following types: [keyword]. Help: Create an index for this key or use a different filter.\"},\"time\":3.639 ...'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lỗi khi xử lý câu hỏi: Unexpected Response: 400 (Bad Request)\n",
      "Raw response content:\n",
      "b'{\"status\":{\"error\":\"Bad request: Index required but not found for \\\\\"Keywords\\\\\" of one of the following types: [keyword]. Help: Create an index for this key or use a different filter.\"},\"time\":3.639 ...'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 329\u001b[39m\n\u001b[32m    325\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mĐã xảy ra lỗi nghiêm trọng: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 302\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    299\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m50\u001b[39m)\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m302\u001b[39m     user_query = \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mCâu hỏi: \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.strip()\n\u001b[32m    304\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m user_query.lower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mquit\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mexit\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mq\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mthoát\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m    305\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCảm ơn bạn đã sử dụng tui : D !!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Desktop\\Advanced-RAG\\env\\Lib\\site-packages\\ipykernel\\kernelbase.py:1275\u001b[39m, in \u001b[36mKernel.raw_input\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m   1273\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1274\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1275\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1276\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1277\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1278\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1279\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1280\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Desktop\\Advanced-RAG\\env\\Lib\\site-packages\\ipykernel\\kernelbase.py:1320\u001b[39m, in \u001b[36mKernel._input_request\u001b[39m\u001b[34m(self, prompt, ident, parent, password)\u001b[39m\n\u001b[32m   1317\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1318\u001b[39m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[32m   1319\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInterrupted by user\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1321\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1322\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.warning(\u001b[33m\"\u001b[39m\u001b[33mInvalid Message:\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Optional\n",
    "\n",
    "from llama_index.core import (\n",
    "    Settings,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    SimpleDirectoryReader, \n",
    "    load_index_from_storage,\n",
    "    PromptTemplate\n",
    ")\n",
    "from llama_index.core.program import LLMTextCompletionProgram\n",
    "from llama_index.core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from llama_index.core.node_parser import HierarchicalNodeParser, get_leaf_nodes\n",
    "from llama_index.core.response_synthesizers import ResponseMode\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.callbacks import LlamaDebugHandler, CallbackManager\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "from llama_index.embeddings.bedrock import BedrockEmbedding\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core.retrievers import AutoMergingRetriever\n",
    "from llama_index.core.vector_stores import ExactMatchFilter, MetadataFilters\n",
    "\n",
    "# Assuming 'caller' module and 'MarketingDocs' class exist\n",
    "# from caller import MarketingDocs \n",
    "\n",
    "import qdrant_client\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Thiết lập logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Định nghĩa các model và hàm hỗ trợ ---\n",
    "\n",
    "class QueryFilters(BaseModel):\n",
    "    \"\"\"Data model for query filters.\"\"\"\n",
    "    categories: List[str] = Field(description=\"List of categories to filter on.\")\n",
    "    keywords: List[str] = Field(description=\"List of keywords to filter on.\")\n",
    "\n",
    "def get_valid_filters_from_db():\n",
    "    # Placeholder: Replace with actual DB query in a real application\n",
    "    return {\n",
    "        \"categories\": [\"Customer Value Analysis\", \"Loyalty & Retention\", \"Case Study\", \"Branding & Positioning\"],\n",
    "        \"keywords\": [\"Customer Lifetime Value (CLV)\", \"Churn Rate Analysis\", \"Marketing ROI\", \"Decile Analysis\"]\n",
    "    }\n",
    "\n",
    "def generate_filters_for_query(query_str: str, llm: any) -> QueryFilters:\n",
    "    \"\"\"Uses an LLM to extract relevant categories and keywords from a user query.\"\"\"\n",
    "    logger.debug(f\"Generating filters for query: {query_str}\")\n",
    "    valid_filters = get_valid_filters_from_db()\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        \"Based on the user's query, identify relevant filters from the available options.\\n\"\n",
    "        \"Respond ONLY with a JSON object. If no filters are relevant, return empty lists.\\n\\n\"\n",
    "        \"Available Categories: {categories}\\n\"\n",
    "        \"Available Keywords: {keywords}\\n\\n\"\n",
    "        \"User Query: \\\"{query_str}\\\"\"\n",
    "    )\n",
    "    \n",
    "    program = LLMTextCompletionProgram.from_defaults(\n",
    "        output_parser=PydanticOutputParser(output_cls=QueryFilters),\n",
    "        prompt=prompt.partial_format(\n",
    "            categories=valid_filters['categories'],\n",
    "            keywords=valid_filters['keywords']\n",
    "        ),\n",
    "        llm=llm,\n",
    "    )\n",
    "    \n",
    "    # Pass the query_str to the program\n",
    "    filter_object = program(query_str=query_str)\n",
    "    logger.info(f\"LLM generated filters: {filter_object.dict()}\")\n",
    "    return filter_object\n",
    "\n",
    "# --- Lớp Wrapper cho Dynamic Query Engine ---\n",
    "\n",
    "class DynamicQueryWrapper:\n",
    "    def __init__(self, index, storage_context, llm, similarity_top_k=30):\n",
    "        self.index = index\n",
    "        self.storage_context = storage_context\n",
    "        self.llm = llm\n",
    "        self.similarity_top_k = similarity_top_k\n",
    "        self.callback_manager = CallbackManager([LlamaDebugHandler(print_trace_on_end=True)])\n",
    "        logger.info(\"DynamicQueryWrapper initialized.\")\n",
    "\n",
    "    def query(self, query_str: str):\n",
    "        \"\"\"\n",
    "        Executes the dynamic retrieval process for the given query.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Executing dynamic query: '{query_str}'\")\n",
    "        \n",
    "        # 1. Generate filters dynamically\n",
    "        filters = generate_filters_for_query(query_str, self.llm)\n",
    "        \n",
    "        # 2. Construct metadata filters list\n",
    "        metadata_filters_list = []\n",
    "        if filters.categories:\n",
    "            metadata_filters_list.extend([ExactMatchFilter(key=\"Category\", value=c) for c in filters.categories])\n",
    "        if filters.keywords:\n",
    "            metadata_filters_list.extend([ExactMatchFilter(key=\"Keywords\", value=k) for k in filters.keywords])\n",
    "        \n",
    "        # 3. Create the base retriever with DYNAMIC filters\n",
    "        base_retriever = self.index.as_retriever(\n",
    "            similarity_top_k=self.similarity_top_k,\n",
    "            filters=MetadataFilters(filters=metadata_filters_list)\n",
    "        )\n",
    "        \n",
    "        # 4. Wrap with AutoMergingRetriever\n",
    "        retriever = AutoMergingRetriever(\n",
    "            base_retriever, \n",
    "            self.storage_context,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        # 5. Create the query engine instance\n",
    "        query_engine_instance = RetrieverQueryEngine.from_args(\n",
    "            retriever=retriever,\n",
    "            llm=self.llm,\n",
    "            response_mode=ResponseMode.TREE_SUMMARIZE,\n",
    "            callback_manager=self.callback_manager\n",
    "        )\n",
    "\n",
    "        # 6. Execute and return the query result\n",
    "        return query_engine_instance.query(query_str)\n",
    "\n",
    "# --- Lớp NodeStorageHandler ---\n",
    "\n",
    "class NodeStorageHandler:\n",
    "    def __init__(self, google_api_key: str = None, \n",
    "                 qdrant_url: str = None, qdrant_api_key: str = None, collection_name: str = \"sailing\"):\n",
    "        # ... (Khởi tạo LLM, Embedding, và Qdrant Client như trong code của bạn) ...\n",
    "        self.collection_name = collection_name\n",
    "        \n",
    "        if google_api_key:\n",
    "            os.getenv[\"GOOGLE_API_KEY\"] = google_api_key\n",
    "        api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "        if not api_key:\n",
    "            logger.warning(\"Chưa có GOOGLE_API_KEY. Vui lòng thiết lập API key để sử dụng Google Gemini models.\")\n",
    "\n",
    "        self.llm = GoogleGenAI(\n",
    "            model_name=\"gemini-2.0-flash\",\n",
    "            temperature=0.1,\n",
    "        )\n",
    "        self.embed_model = BedrockEmbedding(\n",
    "            model_name=\"amazon.titan-embed-text-v2:0\",\n",
    "            aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "            aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "            region_name=os.getenv(\"AWS_REGION\"),\n",
    "        )\n",
    "\n",
    "        Settings.llm = self.llm\n",
    "        Settings.embed_model = self.embed_model\n",
    "\n",
    "        qdrant_url = qdrant_url or os.getenv(\"QDRANT_URL\")\n",
    "        qdrant_api_key = qdrant_api_key or os.getenv(\"QDRANT_API_KEY\")\n",
    "        self.qdrant_client = self._setup_qdrant_client(qdrant_url, qdrant_api_key)\n",
    "        \n",
    "        self._check_connections()\n",
    "        \n",
    "        self.index = None\n",
    "        self.storage_context = None\n",
    "\n",
    "    def _setup_qdrant_client(self, qdrant_url: Optional[str] = None, qdrant_api_key: Optional[str] = None):\n",
    "        # (Implementazione esistente)\n",
    "        logger.info(\"Thiết lập Qdrant client...\")\n",
    "        if qdrant_url and qdrant_api_key:\n",
    "            client = qdrant_client.QdrantClient(url=qdrant_url, api_key=qdrant_api_key, timeout=60)\n",
    "            logger.info(\"Đã kết nối với Qdrant Cloud\")\n",
    "        else:\n",
    "            client = qdrant_client.QdrantClient(path=\"./qdrant_storage\")\n",
    "            logger.info(\"Đã khởi tạo Qdrant local\")\n",
    "        return client\n",
    "\n",
    "    def _check_connections(self):\n",
    "        # (Implementazione esistente)\n",
    "        logger.info(\"Kiểm tra kết nối...\")\n",
    "        try:\n",
    "            collections = self.qdrant_client.get_collections()\n",
    "            logger.info(f\"✅ Qdrant connection OK - {len(collections.collections)} collections\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Qdrant connection failed: {e}\")\n",
    "\n",
    "    def build_automerging_index(self, nodes: List, persist_dir: str = \"./storage\", insert_batch_size: int = 20):\n",
    "        # (Implementazione esistente)\n",
    "        if not nodes and self.index is None:\n",
    "            logger.info(\"Đang thử tải index hiện có...\")\n",
    "            vector_store = QdrantVectorStore(client=self.qdrant_client, collection_name=self.collection_name)\n",
    "            try:\n",
    "                self.storage_context = StorageContext.from_defaults(persist_dir=persist_dir, vector_store=vector_store)\n",
    "                self.index = load_index_from_storage(self.storage_context)\n",
    "                logger.info(\"Đã tải index thành công.\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Không thể tải index từ storage: {e}\")\n",
    "                self.index = None\n",
    "                self.storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "            return self.index\n",
    "\n",
    "        # ... (Phần logic còn lại để thêm nodes và persist) ...\n",
    "\n",
    "        logger.info(\"Xây dựng hoặc cập nhật index...\")\n",
    "        vector_store = QdrantVectorStore(client=self.qdrant_client, collection_name=self.collection_name)\n",
    "\n",
    "        if not self.storage_context:\n",
    "             if os.path.exists(persist_dir):\n",
    "                self.storage_context = StorageContext.from_defaults(persist_dir=persist_dir, vector_store=vector_store)\n",
    "             else:\n",
    "                self.storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "        \n",
    "        if not self.index:\n",
    "            try:\n",
    "                self.index = load_index_from_storage(self.storage_context)\n",
    "            except Exception:\n",
    "                self.index = None\n",
    "        \n",
    "        self.storage_context.docstore.add_documents(nodes)\n",
    "        leaf_nodes = get_leaf_nodes(nodes)\n",
    "\n",
    "        if self.index is None:\n",
    "            self.index = VectorStoreIndex(\n",
    "                nodes=leaf_nodes, storage_context=self.storage_context, show_progress=True\n",
    "            )\n",
    "        else:\n",
    "            self.index.insert_nodes(leaf_nodes, show_progress=True)\n",
    "            \n",
    "        self.storage_context.persist(persist_dir=persist_dir)\n",
    "        logger.info(\"Đã xây dựng/cập nhật xong AutoMerging Index.\")\n",
    "\n",
    "    def create_query_engine(self, persist_dir: str = \"./storage\", similarity_top_k: int = 30):\n",
    "        \"\"\"\n",
    "        Tạo và trả về đối tượng DynamicQueryWrapper, là query engine thực tế.\n",
    "        \"\"\"\n",
    "        logger.info(\"Tạo query engine...\")\n",
    "        \n",
    "        # Đảm bảo index và storage_context đã được load/tạo\n",
    "        if not self.index:\n",
    "            self.build_automerging_index([], persist_dir=persist_dir) # Pass empty list to load existing if it exists\n",
    "        \n",
    "        # Trả về một instance của DynamicQueryWrapper\n",
    "        query_engine = DynamicQueryWrapper(\n",
    "            index=self.index,\n",
    "            storage_context=self.storage_context,\n",
    "            llm=self.llm,\n",
    "            similarity_top_k=similarity_top_k\n",
    "        )\n",
    "        \n",
    "        logger.debug(\"Query engine đã sẵn sàng!\")\n",
    "        return query_engine\n",
    "\n",
    "    def setup_complete_pipeline(self, persist_dir: str = \"./storage\", similarity_top_k: int = 30):\n",
    "        \"\"\"\n",
    "        Thiết lập pipeline hoàn chỉnh từ nodes đến query engine\n",
    "        \"\"\"\n",
    "        logger.info(\"Thiết lập complete pipeline...\")\n",
    "        \n",
    "        all_nodes = [] \n",
    "        \n",
    "        logger.info(\"Xây dựng AutoMerging Index...\")\n",
    "        self.build_automerging_index(all_nodes, persist_dir)\n",
    "        \n",
    "        logger.info(\"Tạo query engine...\")\n",
    "        query_engine = self.create_query_engine(persist_dir, similarity_top_k)\n",
    "        \n",
    "        logger.info(\"Complete pipeline đã sẵn sàng!\")\n",
    "        return query_engine\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Hàm main để testing NodeStorageHandler với MarketingDocs\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"=== KHỞI TẠO NODE STORAGE HANDLER ===\")\n",
    "        # Thay đổi thông tin Qdrant nếu cần\n",
    "        storage_handler = NodeStorageHandler(\n",
    "            collection_name=\"sailing\",\n",
    "            # qdrant_url=\"https://<your-qdrant-url>.cloud.qdrant.io\",\n",
    "            # qdrant_api_key=\"<your-api-key>\"\n",
    "        )\n",
    "\n",
    "        print(\"=== THIẾT LẬP COMPLETE PIPELINE ===\")\n",
    "        # query_engine bây giờ là một instance của DynamicQueryWrapper\n",
    "        query_engine = storage_handler.setup_complete_pipeline()\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"=== CHẾ ĐỘ INTERACTIVE QUERY ===\")\n",
    "        print(\"Nhập câu hỏi của bạn (gõ 'quit' để thoát):\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        while True:\n",
    "            user_query = input(\"\\nCâu hỏi: \").strip()\n",
    "            \n",
    "            if user_query.lower() in ['quit', 'exit', 'q', 'thoát']:\n",
    "                print(\"\\nCảm ơn bạn đã sử dụng tui : D !!\")\n",
    "                break\n",
    "                \n",
    "            if not user_query:\n",
    "                print(\"Vui lòng nhập câu hỏi!\")\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                print(\"\\nĐang tìm kiếm...\")\n",
    "                # Lệnh gọi này sẽ hoạt động vì query_engine là đối tượng DynamicQueryWrapper\n",
    "                response = query_engine.query(user_query) \n",
    "                print(f\"\\n✅ Trả lời:\\n{response}\")\n",
    "                print(\"-\" * 80)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Lỗi khi xử lý câu hỏi: {e}\", exc_info=True)\n",
    "                print(f\"Lỗi khi xử lý câu hỏi: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Lỗi trong main: {e}\", exc_info=True)\n",
    "        print(f\"Đã xảy ra lỗi nghiêm trọng: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e22cbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
